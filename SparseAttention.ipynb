{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBnqnk99JGy0",
        "outputId": "199ffd99-07c4-47e4-d905-5807bc349d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/NLP-Project/chronos-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhdTKLJGLqjr",
        "outputId": "b9026881-3064-4e38-fbdf-2ead36631e90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP-Project/chronos-forecasting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "sys.path.insert(0, os.path.abspath(\"/content/drive/MyDrive/NLP-Project/chronos-forecasting/src\"))"
      ],
      "metadata": {
        "id": "TD3DtpacMvJj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import sys\n",
        "import torch\n",
        "# /content/drive/MyDrive/NLP-Project/chronos-forecasting/\n",
        "def _import_chronos_bits():\n",
        "    \"\"\"\n",
        "    Adjust these imports if your package/module path is different.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from chronos.chronos2.config import Chronos2CoreConfig\n",
        "        from chronos.chronos2.layers import TimeSelfAttention\n",
        "        return Chronos2CoreConfig, TimeSelfAttention\n",
        "    except Exception:\n",
        "        # fallback: if running from repo root with local package name\n",
        "        from chronos.chronos2.config import Chronos2CoreConfig\n",
        "        from chronos.chronos2.layers import TimeSelfAttention\n",
        "        return Chronos2CoreConfig, TimeSelfAttention\n",
        "\n",
        "Chronos2CoreConfig, TimeSelfAttention = _import_chronos_bits()\n",
        "\n",
        "def make_pattern_full_mask(\n",
        "    pad_mask_2d: torch.Tensor,  # [B, S], 1=keep, 0=pad\n",
        "    num_heads: int,\n",
        "    num_output_patches: int,\n",
        "    dtype: torch.dtype,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Build additive 4D mask [B, H, Q, K] that matches the sparse semantics:\n",
        "      - context queries (q < future_start): cannot attend to future keys (k >= future_start)\n",
        "      - future queries (q >= future_start): can attend to all keys\n",
        "      - padding keys masked everywhere\n",
        "    \"\"\"\n",
        "    B, S = pad_mask_2d.shape\n",
        "    finfo_min = torch.finfo(dtype).min\n",
        "\n",
        "    future_start = S - num_output_patches\n",
        "\n",
        "    # base key padding mask: [B, 1, 1, S]\n",
        "    keep = pad_mask_2d.to(torch.bool)\n",
        "    base = torch.zeros((B, 1, 1, S), device=pad_mask_2d.device, dtype=dtype)\n",
        "    base = base.masked_fill(~keep[:, None, None, :], finfo_min)\n",
        "\n",
        "    # pattern mask: [1, 1, Q, K]\n",
        "    pattern = torch.zeros((1, 1, S, S), device=pad_mask_2d.device, dtype=dtype)\n",
        "    if future_start > 0:\n",
        "        pattern[:, :, :future_start, future_start:] = finfo_min\n",
        "\n",
        "    # expand to heads\n",
        "    mask = (base + pattern).expand(B, num_heads, S, S).contiguous()\n",
        "    return mask\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_all():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float32  # keep float32 for strict comparisons\n",
        "\n",
        "    # Small-ish config for fast testing\n",
        "    d_model = 64\n",
        "    d_kv = 16\n",
        "    n_heads = 4\n",
        "\n",
        "    cfg = Chronos2CoreConfig(\n",
        "        d_model=d_model,\n",
        "        d_kv=d_kv,\n",
        "        num_heads=n_heads,\n",
        "        num_layers=1,\n",
        "        dropout_rate=0.0,\n",
        "        attn_implementation=\"sdpa\",\n",
        "    )\n",
        "\n",
        "    B = 2\n",
        "    S = 64\n",
        "    num_output_patches = 8\n",
        "    future_start = S - num_output_patches\n",
        "\n",
        "    # Place REG just before future (matches \"context + REG + future\")\n",
        "    reg_token_index = future_start - 1\n",
        "\n",
        "    # Inputs\n",
        "    x = torch.randn(B, S, d_model, device=device, dtype=dtype)\n",
        "    pos = torch.arange(S, device=device).unsqueeze(0).repeat(B, 1)\n",
        "    pad = torch.ones(B, S, device=device, dtype=dtype)\n",
        "    # Add a little padding at the end\n",
        "    pad[:, -3:] = 0.0\n",
        "\n",
        "    # ---------- 1) Sparse mode: output_attentions must raise ----------\n",
        "    cfg.time_attention_type = \"windowed_future_global\"\n",
        "    cfg.time_local_radius = 4\n",
        "    cfg.time_attention_chunk_size = 8\n",
        "    cfg.time_reg_is_global = False\n",
        "\n",
        "    attn = TimeSelfAttention(cfg).to(device).eval()\n",
        "\n",
        "    try:\n",
        "        _ = attn(x, attention_mask=pad, position_ids=pos,\n",
        "                 num_output_patches=num_output_patches,\n",
        "                 reg_token_index=reg_token_index,\n",
        "                 output_attentions=True)\n",
        "        raise RuntimeError(\"Expected sparse mode to raise with output_attentions=True, but it did not.\")\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    # ---------- 2) No context->future leakage ----------\n",
        "    out1 = attn(x, attention_mask=pad, position_ids=pos,\n",
        "               num_output_patches=num_output_patches,\n",
        "               reg_token_index=reg_token_index,\n",
        "               output_attentions=False).hidden_states\n",
        "\n",
        "    x2 = x.clone()\n",
        "    x2[:, future_start:, :] += torch.randn_like(x2[:, future_start:, :]) * 3.0  # perturb only future tokens\n",
        "\n",
        "    out2 = attn(x2, attention_mask=pad, position_ids=pos,\n",
        "               num_output_patches=num_output_patches,\n",
        "               reg_token_index=reg_token_index,\n",
        "               output_attentions=False).hidden_states\n",
        "\n",
        "    ctx_diff = (out1[:, :future_start, :] - out2[:, :future_start, :]).abs().max().item()\n",
        "    fut_diff = (out1[:, future_start:, :] - out2[:, future_start:, :]).abs().max().item()\n",
        "    assert ctx_diff < 1e-5, f\"Context changed when only future tokens changed (leakage). max diff={ctx_diff}\"\n",
        "    assert fut_diff > 1e-4, f\"Future did not change when future tokens changed (unexpected). max diff={fut_diff}\"\n",
        "\n",
        "    # ---------- 3) Window locality for context queries ----------\n",
        "    cfg.time_local_radius = 2\n",
        "    cfg.time_attention_chunk_size = 7\n",
        "    attn = TimeSelfAttention(cfg).to(device).eval()\n",
        "\n",
        "    outA = attn(x, attention_mask=pad, position_ids=pos,\n",
        "                num_output_patches=num_output_patches,\n",
        "                reg_token_index=reg_token_index).hidden_states\n",
        "\n",
        "    i = 10\n",
        "    j_far = i + 10  # outside radius=2\n",
        "    x3 = x.clone()\n",
        "    x3[:, j_far, :] += torch.randn_like(x3[:, j_far, :]) * 5.0\n",
        "\n",
        "    outB = attn(x3, attention_mask=pad, position_ids=pos,\n",
        "                num_output_patches=num_output_patches,\n",
        "                reg_token_index=reg_token_index).hidden_states\n",
        "\n",
        "    local_diff = (outA[:, i, :] - outB[:, i, :]).abs().max().item()\n",
        "    assert local_diff < 1e-5, f\"Locality broken: position {i} changed due to far token {j_far}. diff={local_diff}\"\n",
        "\n",
        "    # ---------- 4) Future queries are global ----------\n",
        "    # Change the earliest context token, future outputs should change.\n",
        "    x4 = x.clone()\n",
        "    x4[:, 0, :] += torch.randn_like(x4[:, 0, :]) * 5.0\n",
        "\n",
        "    outC = attn(x4, attention_mask=pad, position_ids=pos,\n",
        "                num_output_patches=num_output_patches,\n",
        "                reg_token_index=reg_token_index).hidden_states\n",
        "\n",
        "    fut_global_diff = (outA[:, future_start:, :] - outC[:, future_start:, :]).abs().max().item()\n",
        "    assert fut_global_diff > 1e-4, f\"Future outputs did not react to far context change (not global?). diff={fut_global_diff}\"\n",
        "\n",
        "    # ---------- 5) Padding respected ----------\n",
        "    # Modify padded tokens only; earlier outputs should stay unchanged.\n",
        "    x5 = x.clone()\n",
        "    x5[:, -3:, :] += torch.randn_like(x5[:, -3:, :]) * 10.0  # these are padded keys\n",
        "    outD = attn(x5, attention_mask=pad, position_ids=pos,\n",
        "                num_output_patches=num_output_patches,\n",
        "                reg_token_index=reg_token_index).hidden_states\n",
        "\n",
        "    pad_respect_diff = (outA[:, :-3, :] - outD[:, :-3, :]).abs().max().item()\n",
        "    assert pad_respect_diff < 1e-5, f\"Padding not respected: non-pad outputs changed. diff={pad_respect_diff}\"\n",
        "\n",
        "    # ---------- 6) Chunk-size invariance ----------\n",
        "    cfg.time_attention_chunk_size = 1\n",
        "    attn_cs = TimeSelfAttention(cfg).to(device).eval()\n",
        "    o1 = attn_cs(\n",
        "        x, attention_mask=pad, position_ids=pos,\n",
        "        num_output_patches=num_output_patches,\n",
        "        reg_token_index=reg_token_index\n",
        "    ).hidden_states\n",
        "\n",
        "    # change chunk size on the same module/config\n",
        "    attn_cs.config.time_attention_chunk_size = 16\n",
        "    o2 = attn_cs(\n",
        "        x, attention_mask=pad, position_ids=pos,\n",
        "        num_output_patches=num_output_patches,\n",
        "        reg_token_index=reg_token_index\n",
        "    ).hidden_states\n",
        "\n",
        "    chunk_diff = (o1 - o2).abs().max().item()\n",
        "    assert chunk_diff < 1e-5, f\"Chunk size changed outputs too much. diff={chunk_diff}\"\n",
        "\n",
        "\n",
        "    # ---------- 7) Optional dense equivalence with large radius ----------\n",
        "    # Build a full 4D mask with the same semantics and compare.\n",
        "    cfg_full = Chronos2CoreConfig(\n",
        "        d_model=d_model, d_kv=d_kv, num_heads=n_heads, num_layers=1,\n",
        "        dropout_rate=0.0, attn_implementation=\"sdpa\",\n",
        "    )\n",
        "    cfg_full.time_attention_type = \"full\"\n",
        "\n",
        "    cfg_sparse = Chronos2CoreConfig(\n",
        "        d_model=d_model, d_kv=d_kv, num_heads=n_heads, num_layers=1,\n",
        "        dropout_rate=0.0, attn_implementation=\"sdpa\",\n",
        "    )\n",
        "    cfg_sparse.time_attention_type = \"windowed_future_global\"\n",
        "    cfg_sparse.time_local_radius = S  # big enough to include all context keys\n",
        "    cfg_sparse.time_attention_chunk_size = 8\n",
        "    cfg_sparse.time_reg_is_global = False\n",
        "\n",
        "    dense = TimeSelfAttention(cfg_full).to(device).eval()\n",
        "    sparse = TimeSelfAttention(cfg_sparse).to(device).eval()\n",
        "    sparse.load_state_dict(dense.state_dict())\n",
        "\n",
        "    full_mask = make_pattern_full_mask(pad, n_heads, num_output_patches, dtype=dtype)\n",
        "    od = dense(x, attention_mask=full_mask, position_ids=pos,\n",
        "               num_output_patches=num_output_patches,\n",
        "               reg_token_index=reg_token_index).hidden_states\n",
        "    os = sparse(x, attention_mask=pad, position_ids=pos,\n",
        "                num_output_patches=num_output_patches,\n",
        "                reg_token_index=reg_token_index).hidden_states\n",
        "\n",
        "    eq_diff = (od - os).abs().max().item()\n",
        "    assert eq_diff < 1e-4, f\"Dense vs sparse (large radius) mismatch. diff={eq_diff}\"\n",
        "\n",
        "    print(\"✅ All sparse time-attention sanity tests passed.\")\n",
        "\n",
        "def grad_test():\n",
        "    # quick backward test\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float32\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    cfg = Chronos2CoreConfig(d_model=64, d_kv=16, num_heads=4, num_layers=1, dropout_rate=0.0, attn_implementation=\"sdpa\")\n",
        "    cfg.time_attention_type = \"windowed_future_global\"\n",
        "    cfg.time_local_radius = 2\n",
        "    cfg.time_attention_chunk_size = 8\n",
        "    cfg.time_reg_is_global = False\n",
        "\n",
        "    attn = TimeSelfAttention(cfg).to(device).train()\n",
        "\n",
        "    B, S = 2, 64\n",
        "    num_output_patches = 8\n",
        "    x = torch.randn(B, S, 64, device=device, dtype=dtype, requires_grad=True)\n",
        "    pos = torch.arange(S, device=device).unsqueeze(0).repeat(B, 1)\n",
        "    pad = torch.ones(B, S, device=device, dtype=dtype)\n",
        "\n",
        "    out = attn(x, attention_mask=pad, position_ids=pos,\n",
        "              num_output_patches=num_output_patches,\n",
        "              reg_token_index=(S - num_output_patches - 1)).hidden_states\n",
        "    loss = out.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    # Ensure at least one parameter has gradients\n",
        "    grads = [p.grad for p in attn.parameters() if p.requires_grad]\n",
        "    assert any(g is not None and torch.isfinite(g).all() for g in grads), \"No valid gradients found.\"\n",
        "    print(\"✅ Backward/grad test passed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all()\n",
        "    grad_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W04R8HqDLl9Z",
        "outputId": "ea79e359-cba4-4963-f281-522f1d4f0351"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All sparse time-attention sanity tests passed.\n",
            "✅ Backward/grad test passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import sys\n",
        "import torch\n",
        "# /content/drive/MyDrive/NLP-Project/chronos-forecasting/\n",
        "def _import_chronos_bits():\n",
        "    \"\"\"\n",
        "    Adjust these imports if your package/module path is different.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from chronos.chronos2.config import Chronos2CoreConfig\n",
        "        from chronos.chronos2.layers import TimeSelfAttention\n",
        "        return Chronos2CoreConfig, TimeSelfAttention\n",
        "    except Exception:\n",
        "        # fallback: if running from repo root with local package name\n",
        "        from chronos.chronos2.config import Chronos2CoreConfig\n",
        "        from chronos.chronos2.layers import TimeSelfAttention\n",
        "        return Chronos2CoreConfig, TimeSelfAttention\n",
        "\n",
        "Chronos2CoreConfig, TimeSelfAttention = _import_chronos_bits()"
      ],
      "metadata": {
        "id": "_cQjiGOLYUVJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def _import_chronos():\n",
        "    \"\"\"\n",
        "    Adjust these imports if your paths differ.\n",
        "    The fallback imports assume you're running from the module directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from chronos.chronos2.model import Chronos2Encoder\n",
        "        from chronos.chronos2.config import Chronos2CoreConfig\n",
        "        return Chronos2Encoder, Chronos2CoreConfig\n",
        "    except Exception:\n",
        "        from model import Chronos2Encoder\n",
        "        from config import Chronos2CoreConfig\n",
        "        return Chronos2Encoder, Chronos2CoreConfig\n",
        "\n",
        "\n",
        "Chronos2Encoder, Chronos2CoreConfig = _import_chronos()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_sparse_encoder_does_not_build_4d_mask():\n",
        "    \"\"\"\n",
        "    Ensures Chronos2Encoder.forward() does NOT call the dense mask builder\n",
        "    when time_attention_type == \"windowed_future_global\".\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float32\n",
        "\n",
        "    cfg = Chronos2CoreConfig(\n",
        "        d_model=64,\n",
        "        d_kv=16,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        dropout_rate=0.0,\n",
        "        attn_implementation=\"sdpa\",\n",
        "    )\n",
        "    cfg.time_attention_type = \"windowed_future_global\"\n",
        "    cfg.time_local_radius = 4\n",
        "    cfg.time_attention_chunk_size = 8\n",
        "    cfg.time_reg_is_global = False\n",
        "\n",
        "    enc = Chronos2Encoder(cfg).to(device).eval()\n",
        "\n",
        "    # Monkeypatch the dense mask builder: if it's called, we fail.\n",
        "    called = {\"flag\": False}\n",
        "    orig = enc._expand_and_invert_time_attention_mask\n",
        "\n",
        "    def _trap(*args, **kwargs):\n",
        "        called[\"flag\"] = True\n",
        "        raise AssertionError(\n",
        "            \"_expand_and_invert_time_attention_mask was called in sparse mode \"\n",
        "            \"(should not happen).\"\n",
        "        )\n",
        "\n",
        "    enc._expand_and_invert_time_attention_mask = _trap  # patch\n",
        "\n",
        "    B, S = 2, 64\n",
        "    H = 8  # num_output_patches\n",
        "    x = torch.randn(B, S, cfg.d_model, device=device, dtype=dtype)\n",
        "    pad = torch.ones(B, S, device=device, dtype=dtype)  # 2D padding mask\n",
        "    pos = torch.arange(S, device=device).unsqueeze(0).repeat(B, 1)\n",
        "    group_ids = torch.arange(B, device=device, dtype=torch.long)\n",
        "\n",
        "    # group_time_mask shape depends on your implementation; in your encoder it’s used by GroupSelfAttention\n",
        "    # This is the common shape used in your repo: [B, S, S] boolean-ish mask.\n",
        "    group_time_mask = torch.ones(B, S, S, device=device, dtype=torch.bool)\n",
        "\n",
        "    _ = enc(\n",
        "        inputs_embeds=x,\n",
        "        group_ids=group_ids,\n",
        "        attention_mask=pad,  # MUST be 2D in sparse mode\n",
        "        position_ids=pos,\n",
        "        num_output_patches=H,\n",
        "        reg_token_index=None,\n",
        "        output_attentions=False,\n",
        "    )\n",
        "\n",
        "    assert not called[\"flag\"], \"Dense mask builder was called in sparse mode.\"\n",
        "    enc._expand_and_invert_time_attention_mask = orig  # restore (optional)\n",
        "    print(\"✅ Test 1 passed: sparse encoder did NOT build a 4D time mask.\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_reg_global_no_context_to_future_leak():\n",
        "    \"\"\"\n",
        "    With REG enabled and time_reg_is_global=True, verify:\n",
        "      - modifying ONLY future tokens does not change context outputs (including REG)\n",
        "    This tests that context queries still cannot attend to future keys.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float32\n",
        "\n",
        "    cfg = Chronos2CoreConfig(\n",
        "        d_model=64,\n",
        "        d_kv=16,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        dropout_rate=0.0,\n",
        "        attn_implementation=\"sdpa\",\n",
        "        use_reg_token=True,  # IMPORTANT\n",
        "    )\n",
        "    cfg.time_attention_type = \"windowed_future_global\"\n",
        "    cfg.time_local_radius = 4\n",
        "    cfg.time_attention_chunk_size = 8\n",
        "    cfg.time_reg_is_global = True  # IMPORTANT\n",
        "\n",
        "    enc = Chronos2Encoder(cfg).to(device).eval()\n",
        "\n",
        "    B = 2\n",
        "    S_ctx = 48\n",
        "    H_fut = 8\n",
        "    # Sequence layout: [context tokens] + [REG] + [future tokens]\n",
        "    S = S_ctx + 1 + H_fut\n",
        "    reg_token_index = S_ctx\n",
        "    future_start = reg_token_index + 1\n",
        "\n",
        "    x = torch.randn(B, S, cfg.d_model, device=device, dtype=dtype)\n",
        "    pad = torch.ones(B, S, device=device, dtype=dtype)\n",
        "    pos = torch.arange(S, device=device).unsqueeze(0).repeat(B, 1)\n",
        "    group_ids = torch.arange(B, device=device, dtype=torch.long)\n",
        "    group_time_mask = torch.ones(B, S, S, device=device, dtype=torch.bool)\n",
        "\n",
        "    out1 = enc(\n",
        "        inputs_embeds=x,\n",
        "        group_ids=group_ids,\n",
        "        attention_mask=pad,\n",
        "        position_ids=pos,\n",
        "        num_output_patches=H_fut,\n",
        "        reg_token_index=reg_token_index,\n",
        "        output_attentions=False,\n",
        "    ).last_hidden_state  # adjust field name if yours differs\n",
        "\n",
        "    x2 = x.clone()\n",
        "    x2[:, future_start:, :] += torch.randn_like(x2[:, future_start:, :]) * 3.0  # perturb ONLY future tokens\n",
        "\n",
        "    out2 = enc(\n",
        "        inputs_embeds=x2,\n",
        "        group_ids=group_ids,\n",
        "        attention_mask=pad,\n",
        "        position_ids=pos,\n",
        "        num_output_patches=H_fut,\n",
        "        reg_token_index=reg_token_index,\n",
        "        output_attentions=False,\n",
        "    ).last_hidden_state\n",
        "\n",
        "    # Context includes [0 .. reg_token_index] (context + REG)\n",
        "    ctx1 = out1[:, :future_start, :]\n",
        "    ctx2 = out2[:, :future_start, :]\n",
        "    diff = (ctx1 - ctx2).abs().max().item()\n",
        "\n",
        "    assert diff < 1e-5, f\"Context/REG changed when only future tokens changed (leak). diff={diff}\"\n",
        "    print(\"✅ Test 2 passed: REG-global still has no context→future leakage.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_sparse_encoder_does_not_build_4d_mask()\n",
        "    test_reg_global_no_context_to_future_leak()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKvqjvy9UxeU",
        "outputId": "ef913527-d387-4ef0-97b6-93b94aa0cf5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test 1 passed: sparse encoder did NOT build a 4D time mask.\n",
            "✅ Test 2 passed: REG-global still has no context→future leakage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "\n",
        "\n",
        "def _import_chronos():\n",
        "    \"\"\"\n",
        "    Adjust these imports if your paths differ.\n",
        "    Fallback assumes running from the module folder.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from chronos.chronos2.model import Chronos2Encoder\n",
        "        from chronos.chronos2.config import Chronos2CoreConfig\n",
        "        return Chronos2Encoder, Chronos2CoreConfig\n",
        "    except Exception:\n",
        "        from chronos.chronos2.model import Chronos2Encoder\n",
        "        from chronos.chronos2.config import Chronos2CoreConfig\n",
        "        return Chronos2Encoder, Chronos2CoreConfig\n",
        "\n",
        "\n",
        "Chronos2Encoder, Chronos2CoreConfig = _import_chronos()\n",
        "\n",
        "\n",
        "def _cuda_mem_str(x: int) -> str:\n",
        "    return f\"{x/1024**2:.1f} MiB\"\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def long_context_forward_benchmark(seq_len: int, num_output_patches: int, radius: int, chunk: int):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "    cfg = Chronos2CoreConfig(\n",
        "        d_model=512,          # closer to realistic\n",
        "        d_kv=64,\n",
        "        num_heads=8,\n",
        "        num_layers=4,         # keep modest for sanity test\n",
        "        dropout_rate=0.0,\n",
        "        attn_implementation=\"sdpa\",\n",
        "        use_reg_token=False,\n",
        "    )\n",
        "    cfg.time_attention_type = \"windowed_future_global\"\n",
        "    cfg.time_local_radius = radius\n",
        "    cfg.time_attention_chunk_size = chunk\n",
        "    cfg.time_reg_is_global = False\n",
        "\n",
        "    enc = Chronos2Encoder(cfg).to(device).eval()\n",
        "\n",
        "    B = 1\n",
        "    x = torch.randn(B, seq_len, cfg.d_model, device=device, dtype=dtype)\n",
        "    pad = torch.ones(B, seq_len, device=device, dtype=dtype)\n",
        "    pos = torch.arange(seq_len, device=device).unsqueeze(0)\n",
        "    group_ids = torch.zeros(B, device=device, dtype=torch.long)\n",
        "\n",
        "    # Warmup\n",
        "    _ = enc(\n",
        "        inputs_embeds=x,\n",
        "        group_ids=group_ids,\n",
        "        attention_mask=pad,\n",
        "        position_ids=pos,\n",
        "        num_output_patches=num_output_patches,\n",
        "        reg_token_index=None,\n",
        "        output_attentions=False,\n",
        "    )\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    out = enc(\n",
        "        inputs_embeds=x,\n",
        "        group_ids=group_ids,\n",
        "        attention_mask=pad,\n",
        "        position_ids=pos,\n",
        "        num_output_patches=num_output_patches,\n",
        "        reg_token_index=None,\n",
        "        output_attentions=False,\n",
        "    )\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    # Try to access output tensor robustly\n",
        "    y = getattr(out, \"last_hidden_state\", None)\n",
        "    if y is None:\n",
        "        y = getattr(out, \"hidden_states\", None)\n",
        "    if y is None:\n",
        "        # fallback: common naming in some codebases\n",
        "        y = getattr(out, \"final_hidden_state\", None)\n",
        "    if y is None:\n",
        "        raise RuntimeError(\"Could not find encoder output tensor on Chronos2EncoderOutput\")\n",
        "\n",
        "    peak = None\n",
        "    if device == \"cuda\":\n",
        "        peak = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    print(f\"✅ Forward OK on device={device}, dtype={dtype}\")\n",
        "    print(f\"   seq_len={seq_len}, num_output_patches={num_output_patches}, radius={radius}, chunk={chunk}\")\n",
        "    print(f\"   output shape: {tuple(y.shape)}\")\n",
        "    print(f\"   time: {(t1 - t0):.3f}s\")\n",
        "    if peak is not None:\n",
        "        print(f\"   peak CUDA allocated: {_cuda_mem_str(peak)}\")\n",
        "\n",
        "\n",
        "def long_context_backward_benchmark(seq_len: int, num_output_patches: int, radius: int, chunk: int):\n",
        "    \"\"\"\n",
        "    Backward test is the real memory killer; run it once at batch=1.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "    cfg = Chronos2CoreConfig(\n",
        "        d_model=512,\n",
        "        d_kv=64,\n",
        "        num_heads=8,\n",
        "        num_layers=4,\n",
        "        dropout_rate=0.0,\n",
        "        attn_implementation=\"sdpa\",\n",
        "        use_reg_token=False,\n",
        "    )\n",
        "    cfg.time_attention_type = \"windowed_future_global\"\n",
        "    cfg.time_local_radius = radius\n",
        "    cfg.time_attention_chunk_size = chunk\n",
        "    cfg.time_reg_is_global = False\n",
        "\n",
        "    enc = Chronos2Encoder(cfg).to(device).train()\n",
        "\n",
        "    B = 1\n",
        "    x = torch.randn(B, seq_len, cfg.d_model, device=device, dtype=dtype, requires_grad=True)\n",
        "    pad = torch.ones(B, seq_len, device=device, dtype=dtype)\n",
        "    pos = torch.arange(seq_len, device=device).unsqueeze(0)\n",
        "    group_ids = torch.zeros(B, device=device, dtype=torch.long)\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    out = enc(\n",
        "        inputs_embeds=x,\n",
        "        group_ids=group_ids,\n",
        "        attention_mask=pad,\n",
        "        position_ids=pos,\n",
        "        num_output_patches=num_output_patches,\n",
        "        reg_token_index=None,\n",
        "        output_attentions=False,\n",
        "    )\n",
        "    y = getattr(out, \"last_hidden_state\", None) or getattr(out, \"hidden_states\", None) or getattr(out, \"final_hidden_state\", None)\n",
        "    if y is None:\n",
        "        raise RuntimeError(\"Could not find encoder output tensor on Chronos2EncoderOutput\")\n",
        "\n",
        "    loss = y.float().mean()\n",
        "    loss.backward()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    peak = None\n",
        "    if device == \"cuda\":\n",
        "        peak = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    print(f\"✅ Backward OK on device={device}, dtype={dtype}\")\n",
        "    print(f\"   seq_len={seq_len}, num_output_patches={num_output_patches}, radius={radius}, chunk={chunk}\")\n",
        "    print(f\"   time (fwd+bwd): {(t1 - t0):.3f}s\")\n",
        "    if peak is not None:\n",
        "        print(f\"   peak CUDA allocated: {_cuda_mem_str(peak)}\")\n",
        "\n",
        "\n",
        "def config_propagation_sanity(training_step_fn, model, expected: str = \"windowed_future_global\", steps: int = 3):\n",
        "    \"\"\"\n",
        "    Generic config propagation check: call this inside any training loop.\n",
        "    It asserts the config doesn't silently revert to full.\n",
        "    \"\"\"\n",
        "    for step in range(steps):\n",
        "        tat = None\n",
        "        # Common locations depending on how you store it\n",
        "        if hasattr(model, \"chronos_config\") and hasattr(model.chronos_config, \"time_attention_type\"):\n",
        "            tat = model.chronos_config.time_attention_type\n",
        "        elif hasattr(model, \"config\") and hasattr(model.config, \"chronos_config\"):\n",
        "            # HF-style dict storage\n",
        "            cc = model.config.chronos_config\n",
        "            if isinstance(cc, dict):\n",
        "                tat = cc.get(\"time_attention_type\", None)\n",
        "            else:\n",
        "                tat = getattr(cc, \"time_attention_type\", None)\n",
        "\n",
        "        print(f\"[step {step}] time_attention_type = {tat}\")\n",
        "        assert tat == expected, f\"time_attention_type changed/reverted (got {tat}, expected {expected})\"\n",
        "\n",
        "        training_step_fn(step)\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--seq\", type=int, default=8192, help=\"Sequence length (e.g., 8192 or 16384)\")\n",
        "    ap.add_argument(\"--out\", type=int, default=64, help=\"num_output_patches (future tokens)\")\n",
        "    ap.add_argument(\"--radius\", type=int, default=128, help=\"time_local_radius\")\n",
        "    ap.add_argument(\"--chunk\", type=int, default=32, help=\"time_attention_chunk_size\")\n",
        "    ap.add_argument(\"--no-backward\", action=\"store_true\", help=\"Skip backward test\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    long_context_forward_benchmark(args.seq, args.out, args.radius, args.chunk)\n",
        "    if not args.no_backward:\n",
        "        long_context_backward_benchmark(args.seq, args.out, args.radius, args.chunk)\n",
        "\n",
        "    print(\"\\nConfig propagation sanity: integrate this into your training loop.\")\n",
        "    print(\"Example usage is printed below.\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Example:\\n\"\n",
        "        \"  # inside your training script\\n\"\n",
        "        \"  def one_step(step):\\n\"\n",
        "        \"      ...  # run one optimizer step\\n\"\n",
        "        \"  config_propagation_sanity(one_step, model, expected='windowed_future_global', steps=3)\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "uRMrOfGQWQN2",
        "outputId": "e0af3b84-a1c1-48a4-9ba5-38ca59b9c074"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--seq SEQ] [--out OUT] [--radius RADIUS]\n",
            "                                [--chunk CHUNK] [--no-backward]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-a1294821-efcf-45f8-b2e7-006d0031e89d.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect, chronos.chronos2.layers as L\n",
        "print(\"torch.gather(k_ctx) in live code?\",\n",
        "      \"torch.gather(k_ctx\" in inspect.getsource(L.TimeSelfAttention._windowed_future_global_attention))\n"
      ],
      "metadata": {
        "id": "k_oOk17LRQUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import chronos.chronos2.layers as L\n",
        "importlib.reload(L)\n",
        "\n",
        "# then re-import the class you test\n",
        "from chronos.chronos2.layers import TimeSelfAttention"
      ],
      "metadata": {
        "id": "cvvqY3yTRTiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --seq 8192 --out 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3cSaKryXAh8",
        "outputId": "c5d6afa4-68ad-4668-960e-a7868ea20db9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-15 10:15:44.381607: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765793744.401036    1670 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765793744.406919    1670 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765793744.421897    1670 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765793744.421923    1670 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765793744.421927    1670 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765793744.421931    1670 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-15 10:15:44.428738: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✅ Forward OK on device=cuda, dtype=torch.float16\n",
            "   seq_len=8192, num_output_patches=64, radius=128, chunk=32\n",
            "   output shape: (1, 8192, 512)\n",
            "   time: 0.863s\n",
            "   peak CUDA allocated: 288.2 MiB\n",
            "✅ Backward OK on device=cuda, dtype=torch.float16\n",
            "   seq_len=8192, num_output_patches=64, radius=128, chunk=32\n",
            "   time (fwd+bwd): 4.254s\n",
            "   peak CUDA allocated: 1686.8 MiB\n",
            "\n",
            "Config propagation sanity: integrate this into your training loop.\n",
            "Example usage is printed below.\n",
            "\n",
            "Example:\n",
            "  # inside your training script\n",
            "  def one_step(step):\n",
            "      ...  # run one optimizer step\n",
            "  config_propagation_sanity(one_step, model, expected='windowed_future_global', steps=3)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}